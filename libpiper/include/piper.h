#ifndef PIPER_H_\n#define PIPER_H_\n\n#include <stdbool.h>\n#include <stddef.h>\n#include <stdint.h>\n\n// Use compatibility header for platforms without uchar.h\n#if defined(__ANDROID__) || defined(__APPLE__) || !__has_include(<uchar.h>)\n    #include "uchar_compat.h"\n#else\n    #include <uchar.h>\n#endif\n\n#ifdef __cplusplus\nextern "C" {\n#endif\n\n#define PIPER_OK 0\n#define PIPER_DONE 1\n#define PIPER_ERR_GENERIC -1\n\n/**\n * \brief Text-to-speech synthesizer.\n */\ntypedef struct piper_synthesizer piper_synthesizer;\n\n/**\n * \brief Chunk of synthesized audio samples.\n */\ntypedef struct piper_audio_chunk {\n  /**\n   * \brief Raw samples returned from the voice model.\n   */\n  const float *samples;\n\n  /**\n   * \brief Number of samples in the audio chunk.\n   */\n  size_t num_samples;\n\n  /**\n   * \brief Sample rate in Hertz.\n   */\n  int sample_rate;\n\n  /**\n   * \brief True if this is the last audio chunk.\n   */\n  bool is_last;\n\n  /**\n   * \brief Phoneme codepoints that produced this audio chunk, aligned with ids.\n   *\n   * Phonemes will look like [p1, p1, 0, p2, p2, 0, ...] where the same phoneme\n   * codepoint is repeated for each id from that phoneme (usually just one id\n   * plus pad).\n   *\n   * Groups of repeated codepoints are separated by a 0 so that alignments can\n   * be attributed to the correct phoneme. This is accomplished by:\n   *\n   * 1. Read N (repeated) codepoints from phonemes until a 0 is reached (or end)\n   * 2. The next N phoneme ids correspond to that phoneme\n   * 3. The next N alignments (sample counts) correspond to that phoneme\n   * 4. Advance your iterators in the phoneme id and alignment arrays by N\n   * 5. Repeat\n   */\n  const char32_t *phonemes;\n\n  /**\n   * \brief Number of codepoints in phonemes.\n   */\n  size_t num_phonemes;\n\n  /**\n   * \brief Phoneme ids that produced this audio chunk.\n   *\n   * Ids will look like [1, 0, id1, 0, id2, 0, ..., 2] where:\n   * 0 = pad\n   * 1 = beginning of sentence\n   * 2 = end of sentence\n   */\n  const int *phoneme_ids;\n\n  /**\n   * \brief Number of ids in phoneme_ids.\n   */\n  size_t num_phoneme_ids;\n\n  /**\n   * \brief Audio sample count for each phoneme id.\n   *\n   * This includes the meta ids:\n   * 0 = pad\n   * 1 = beginning of sentence\n   * 2 = end of sentence\n   *\n   * Use the phonemes array to align these sample counts with actual phonemes.\n   */\n  const int *alignments;\n\n  /**\n   * \brief Number of alignments.\n   *\n   * This should be the same as num_phoneme_ids.\n   */\n  size_t num_alignments;\n} piper_audio_chunk;\n\n/**\n * \brief Options for synthesis.\n *\n * \sa \ref piper_default_synthesize_options\n */\ntypedef struct piper_synthesize_options {\n  /**\n   * \brief Id of speaker to use (multi-speaker models only).\n   *\n   * Id 0 is the first speaker.\n   */\n  int speaker_id;\n\n  /**\n   * \brief How fast the text is spoken.\n   *\n   * A length scale of 0.5 means to speak twice as fast.\n   * A length scale of 2.0 means to speak twice as slow.\n   * The default is 1.0.\n   */\n  float length_scale;\n\n  /**\n   * \brief Controls how much noise is added during synthesis.\n   *\n   * The best value depends on the voice.\n   * For single speaker models, a value of 0.667 is usually good.\n   * For multi-speaker models, a value of 0.333 is usually good.\n   */\n  float noise_scale;\n\n  /**\n   * \brief Controls how much phonemes vary in length during synthesis.\n   *\n   * The best value depends on the voice.\n   * For single speaker models, a value of 0.8 is usually good.\n   * For multi-speaker models, a value of 0.333 is usually good.\n   */\n  float noise_w_scale;\n} piper_synthesize_options;\n\n/**\n * \brief Create a Piper text-to-speech synthesizer from a voice model.\n *\n * \param model_path path to ONNX voice model file.\n *\n * \param config_path path to JSON voice config file or NULL if it's the\n * model_path + .json.\n *\n * \param espeak_data_path path to the espeak-ng data\n * directory.\n *\n * \return a Piper text-to-speech synthesizer for the voice model.\n */\npiper_synthesizer *piper_create(const char *model_path, const char *config_path,\n                                const char *espeak_data_path);\n\n/**\n * \brief Free resources for Piper synthesizer.\n *\n * \param synth Piper synthesizer.\n */\nvoid piper_free(piper_synthesizer *synth);\n\n/**\n * \brief Get the default synthesis options for a Piper synthesizer.\n *\n * \param synth Piper synthesizer.\n *\n * \return synthesis options from voice config.\n */\npiper_synthesize_options\npiper_default_synthesize_options(piper_synthesizer *synth);\n\n/**\n * \brief Start text-to-speech synthesis.\n *\n * \param synth Piper synthesizer.\n *\n * \param text text to synthesize into audio.\n *\n * \param options synthesis options or NULL for defaults.\n *\n * \sa \ref piper_synthesize_next\n *\n * \return PIPER_OK or error code.\n */\nint piper_synthesize_start(piper_synthesizer *synth, const char *text,\n                           const piper_synthesize_options *options);\n\n/**\n * \brief Synthesize next chunk of audio.\n *\n * \param synth Piper synthesizer.\n *\n * \param chunk audio chunk to fill.\n *\n * piper_synthesize_start must be called before this function.\n * Each call to piper_synthesize_next will fill the audio chunk, invalidating\n * the memory of the previous chunk.\n * The final audio chunk will have is_last = true.\n * A return value of PIPER_DONE indicates that synthesis is complete.\n *\n * \sa \ref piper_synthesize_start\n *\n * \return PIPER_DONE when complete, otherwise PIPER_OK or error code.\n */\nint piper_synthesize_next(piper_synthesizer *synth, piper_audio_chunk *chunk);\n\n/**\n * \brief Get the sample rate of the loaded voice.\n *\n * \param synth Piper synthesizer.\n *\n * \return Sample rate in Hertz, or 0 if synth is NULL.\n */\nint piper_get_sample_rate(piper_synthesizer *synth);\n\n#ifdef __cplusplus\n} // extern "C"\n#endif\n\n#endif // PIPER_H_\n